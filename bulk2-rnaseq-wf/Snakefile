import sys

sys.path.insert(0, srcdir('../lcdb-wf'))
import os
from pathlib import Path
from textwrap import dedent
import yaml
import tempfile
import pandas as pd
from lib import common, cluster_specific, utils, helpers, aligners
from lib.patterns_targets import RNASeqConfig

# ----------------------------------------------------------------------------
#
# Search for the string "NOTE:" to look for points of configuration that might
# be helpful for your experiment.
#
# ----------------------------------------------------------------------------

# Only use default if no configfile specified on the command line with
# --configfile
if not workflow.overwrite_configfile:
    configfile: 'config/config.yaml'
else:
    configfile: workflow.overwrite_configfile

include: '../lcdb-wf/workflows/references/Snakefile'
shell.prefix(
    'set -euo pipefail; export R_PROFILE_USER=; export TMPDIR={};'
    .format(cluster_specific.tempdir_for_biowulf())
)
shell.executable('/bin/bash')

config = common.load_config(config)

c = RNASeqConfig(config, config.get('patterns', 'config/rnaseq_patterns.yaml'))

wildcard_constraints:
    n = '[1,2]'


def wrapper_for(path):
    return 'file:' + os.path.join('../', 'wrappers', path)

# ----------------------------------------------------------------------------
# RULES
# ----------------------------------------------------------------------------


localrules: symlinks, symlink_targets, symlink_bigwigs

# See "patterns and targets" in the documentation for what's going on here.
final_targets = utils.flatten((
    c.targets['bam'],
    utils.flatten(c.targets['fastqc']),
    utils.flatten(c.targets['libsizes']),
    [c.targets['fastq_screen']],
    [c.targets['libsizes_table']],
    [c.targets['rrna_percentages_table']],
    [c.targets['multiqc']],
    utils.flatten(c.targets['featurecounts']),
    [c.targets['agg_featurecounts']],
    [c.targets['tpm_featurecounts']],
    utils.flatten(c.targets['rrna']),
    utils.flatten(c.targets['markduplicates']),
    utils.flatten(c.targets['salmon']),
    utils.flatten(c.targets['dupradar']),
    utils.flatten(c.targets['preseq']),
    utils.flatten(c.targets['rseqc']),
    utils.flatten(c.targets['collectrnaseqmetrics']),
    utils.flatten(c.targets['bigwig']),
    utils.flatten(c.targets['intergenic']),
    [c.targets['featurecounts_intergenic']],
    [c.targets['agg_intergenic_featurecounts']],
    [c.targets['ercc_annotation']],
    [
        '../output/bulk2-rnaseq-wf/deg/larval_testis_biased_fbgns.pkl',
        '../output/bulk2-rnaseq-wf/deg/larva_ovary_biased_fbgns.pkl',
        '../output/bulk2-rnaseq-wf/deg/larval_ns_biased_fbgns.pkl',
    ],
))

# Special case: all samples are single-end
if all(c.sampletable.iloc[:, 0].apply(
    lambda x: not common.is_paired_end(c.sampletable, x))
):
    ALL_SE = True
    final_targets = [i.replace('{n}', '1') for i in final_targets]
else:
    ALL_SE = False

rule targets:
    """
    Final targets to create
    """
    input: final_targets

if 'orig_filename' in c.sampletable.columns:

    # Convert the sampletable to be indexed by the first column, for
    # convenience in generating the input/output filenames.
    _st = c.sampletable.set_index(c.sampletable.columns[0])

    def orig_for_sample(wc):
        """
        Given a sample, returns either one or two original fastq files
        depending on whether the library was single- or paired-end.
        """
        row = _st.loc[wc.sample]
        res = [row['orig_filename']]
        try:
            r2 = row['orig_filename_R2']
            if isinstance(r2, str):
                res.append(row['orig_filename_R2'])
        except KeyError:
            pass
        return res


    rule symlinks:
        """
        Symlinks files over from original filename
        """
        input:
            orig_for_sample
        output:
            c.patterns['fastq']
        wildcard_constraints:
            n="\d+"
        resources:
             mem_gb=lambda wildcards, attempt: attempt * 1,
             time_hr=lambda wildcards, attempt: attempt * 1
        run:

            assert len(output) == 1
            if wildcards.n == '1':
                src = input[0]
            elif wildcards.n == '2':
                src = input[1]
            else:
                raise ValueError("n={}, only 1 and 2 supported".format(wildcards.n))

            utils.make_relative_symlink(src, output[0])


    rule symlink_targets:
        input: c.targets['fastq']
        resources:
             mem_gb=lambda wildcards, attempt: attempt * 1,
             time_hr=lambda wildcards, attempt: attempt * 1


def render_r1_r2(pattern):
    if ALL_SE:
        return expand(pattern, sample='{sample}', n=[1])
    return expand(pattern, sample='{sample}', n=[1, 2])


if 'Run' in c.sampletable.columns and sum(c.sampletable['Run'].str.startswith('SRR')) > 0:

    # Convert the sampletable to be indexed by the first column, for
    # convenience in generating the input/output filenames.
    _st = c.sampletable.set_index(c.sampletable.columns[0])

    rule fastq_dump:
        output:
            fastq=render_r1_r2(c.patterns['fastq'])
        resources:
             mem_gb=lambda wildcards, attempt: attempt * 1,
             time_hr=lambda wildcards, attempt: attempt * 1
        run:
            srr = _st.loc[wildcards.sample, 'Run']

            # Two different paths depending on the layout. In both cases, we
            # want to avoid creating the final output until the very end, to
            # avoid incomplete downloads.
            if common.is_paired_end(c.sampletable, wildcards.sample):

                # For PE we need to use --split-files, which also means using
                # the slower --gzip
                shell(
                    'fastq-dump '
                    '{srr} '
                    '--gzip '
                    '--split-files '
                    # '-X 100000 ' # [TEST SETTINGS]
                )

                # The filenames are predictable, so we can move them as needd.
                shell('mv {srr}_1.fastq.gz {output[0]}')
                shell('mv {srr}_2.fastq.gz {output[1]}')

            else:
                # For SE, we can use the faster stdout | gzip, and move it
                # directly when done.
                shell(
                    'fastq-dump '
                    '{srr} '
                    '-Z '
                    # '-X 100000 ' # [TEST SETTINGS]
                    '| gzip -c > {output[0]}.tmp '
                    '&& mv {output[0]}.tmp {output[0]} '
                )
                if not ALL_SE:
                    shell('touch {output[1]}')


rule cutadapt:
    """
    Run cutadapt
    """
    input:
        fastq=common.fill_r1_r2(c.sampletable, c.patterns['fastq'])
    output:
        fastq=render_r1_r2(c.patterns['cutadapt'])
    log:
        render_r1_r2(c.patterns['cutadapt'])[0] + '.log'
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 2
    run:
        paired = len(input) == 2

        # NOTE: Change cutadapt params here


        if paired:
            shell(
                "cutadapt "
                "{input.fastq[0]} "
                "{input.fastq[1]} "
                "-o {output[0]} "
                "-p {output[1]} "
                "-a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA "
                "-A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT "
                '-q 20 '
                '--minimum-length 25 '
                "&> {log}"
            )
        else:
            shell(
                "cutadapt "
                "{input.fastq[0]} "
                "-o {output[0]} "
                "-a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA "
                '-q 20 '
                '--minimum-length 25 '
                "&> {log}"
            )
            if not ALL_SE:
                shell('touch {output[1]}')


rule fastqc:
    """
    Run FastQC
    """
    input: '{sample_dir}/{sample}/{sample}{suffix}'
    output:
        html='{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.html',
        zip='{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.zip',
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 2
    script:
        wrapper_for('fastqc/wrapper.py')


if config['aligner']['index'] == 'hisat2':
    rule hisat2:
        """
        Map reads with HISAT2
        """
        input:
            fastq=common.fill_r1_r2(c.sampletable, c.patterns['cutadapt']),
            index=[c.refdict[c.organism][config['aligner']['tag']]['hisat2']]
        output:
            bam=c.patterns['bam']
        log:
            c.patterns['bam'] + '.log'
        threads: 6
        resources:
            mem_gb=lambda wildcards, attempt: attempt * 8,
            time_hr=lambda wildcards, attempt: attempt * 4
        run:
            prefix = aligners.prefix_from_bowtie2_index(input.index)
            sam = output.bam.replace('.bam', '.sam')

            # If two fastqs were provided, assume paired-end mode; otherwise single-end
            if isinstance(input.fastq, str) or len(input.fastq) == 1:
                fastqs = '-U {0} '.format(input.fastq)
            else:
                assert len(input.fastq) == 2
                fastqs = '-1 {0} -2 {1} '.format(*input.fastq)

            shell(
                "hisat2 "
                "-x {prefix} "
                "{fastqs} "
                '--no-unal '
                "--threads {threads} "
                "-S {sam} "
                "> {log} 2>&1"
            )

            shell(
                "samtools view -Sb {sam} "
                "| samtools sort - -o {output.bam} -O BAM "
                "&& rm {sam}"
            )


rule rRNA:
    """
    Map reads with bowtie2 to the rRNA reference
    """
    input:
        fastq=common.fill_r1_r2(c.sampletable, c.patterns['cutadapt'], r1_only=True),
        index=[c.refdict[c.organism][config['rrna']['tag']]['bowtie2']]
    output:
        bam=c.patterns['rrna']['bam']
    log:
        c.patterns['rrna']['bam'] + '.log'
    params:
        # NOTE: we'd likely only want to report a single alignment for rRNA
        # screening
        bowtie2_extra='-k 1',
        samtools_view_extra='-F 0x04'
    threads: 6
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 8,
        time_hr=lambda wildcards, attempt: attempt * 4
    run:
        prefix = aligners.prefix_from_bowtie2_index(input.index)
        sam = output.bam.replace('.bam', '.sam')

        shell(
            "bowtie2 "
            "-x {prefix} "
            "-U {input.fastq} "
            '-k 1 '       # NOTE: we only care if >=1 mapped
            '--no-unal '  # NOTE: suppress unaligned reads
            "--threads {threads} "
            "-S {sam} "
            "> {log} 2>&1"
        )

        shell(
            "samtools view -Sb {sam} "
            "| samtools sort - -o {output.bam} -O BAM "
            "&& rm {sam}"
        )


rule fastq_count:
    """
    Count reads in a FASTQ file
    """
    input:
        fastq='{sample_dir}/{sample}/{sample}{suffix}.fastq.gz'
    output:
        count='{sample_dir}/{sample}/{sample}{suffix}.fastq.gz.libsize'
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 1
    shell:
        'zcat {input} | echo $((`wc -l`/4)) > {output}'


rule bam_count:
    """
    Count reads in a BAM file
    """
    input:
        bam='{sample_dir}/{sample}/{suffix}.bam'
    output:
        count='{sample_dir}/{sample}/{suffix}.bam.libsize'
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 1
    shell:
        'samtools view -c {input} > {output}'


rule bam_index:
    """
    Index a BAM
    """
    input:
        bam='{prefix}.bam'
    output:
        bai='{prefix}.bam.bai'
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 1
    shell:
        'samtools index {input} {output}'


def fastq_screen_references():
    """
    Returns the Bowtie2 indexes for the configured references from the
    `fastq_screen:` section of the config
    """
    refs = {}
    for i in config['fastq_screen']:
        refs[i['label']] = c.refdict[i['organism']][i['tag']]['bowtie2']
    return refs


rule fastq_screen:
    """
    Run fastq_screen to look for contamination from other genomes
    """
    input:
        **fastq_screen_references(),
        fastq=common.fill_r1_r2(c.sampletable, rules.cutadapt.output.fastq, r1_only=True),
    output:
        txt=c.patterns['fastq_screen']
    log:
        c.patterns['fastq_screen'] + '.log'
    params: subset=100000
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 8,
        time_hr=lambda wildcards, attempt: attempt * 2
    script:
        wrapper_for('fastq_screen/wrapper.py')


rule featurecounts:
    """
    Count reads in annotations with featureCounts from the subread package
    """
    input:
        annotation=c.refdict[c.organism][config['gtf']['tag']]['gtf'],
        bam=c.patterns['bam']
    output:
        counts='{sample_dir}/{sample}/{sample}.cutadapt.bam.featurecounts.{stranded}.txt'
    wildcard_constraints:
        stranded="s\d"
    log:
        '{sample_dir}/{sample}/{sample}.cutadapt.bam.featurecounts.{stranded}.txt.log'
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 2
    run:
        # NOTE:
        # For paired-end reads, you probably want -p to count fragments rather
        # than individual reads (which may be on opposite strands from each
        # other).
        #
        # By default, this is automatically detected based on the layout column
        # of the sampletable. If you do not want `-p` added automatically for
        # PE samples, change this to False:
        #
        if common.is_paired_end(c.sampletable, wildcards.sample):
            p_arg = '-p'
        else:
            p_arg = ''

        shell(

            'featureCounts '

            # NOTE:
            # By default, this rule runs three times, using a different strand
            # setting each time. The strand argument (-s0, -s1, -s2) to
            # featureCounts is pulled straight from the wildcards, and the
            # corresponding filenames have been set up in rnaseq_patterns.yaml.
            #
            # You probably do NOT want to add your own -s argument below, however
            # other arguments might be useful. For example, for nascent RNA-seq,
            # add '-t gene -g gene_id -f '
            '-{wildcards.stranded} '

            '{p_arg} '
            '-T {threads} '
            '-a {input.annotation} '
            '-o {output.counts} '
            '{input.bam} '
            '&> {log}'
        )


rule rrna_libsizes_table:
    """
    Aggregate rRNA counts into a table
    """
    input:
        rrna=c.targets['rrna']['libsize'],
        fastq=c.targets['libsizes']['cutadapt']
    output:
        json=c.patterns['rrna_percentages_yaml'],
        tsv=c.patterns['rrna_percentages_table']
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 2
    run:
        def rrna_sample(f):
            return helpers.extract_wildcards(c.patterns['rrna']['libsize'], f)['sample']

        def sample(f):
            return helpers.extract_wildcards(c.patterns['libsizes']['cutadapt'], f)['sample']

        def million(f):
            return float(open(f).read()) / 1e6

        rrna = sorted(input.rrna, key=rrna_sample)
        fastq = sorted(input.fastq, key=sample)
        samples = list(map(rrna_sample, rrna))
        rrna_m = list(map(million, rrna))
        fastq_m = list(map(million, fastq))

        df = pd.DataFrame(dict(
            sample=samples,
            million_reads_rRNA=rrna_m,
            million_reads_fastq=fastq_m,
        ))
        df = df.set_index('sample')
        df['rRNA_percentage'] = df.million_reads_rRNA / df.million_reads_fastq * 100

        df[['million_reads_fastq', 'million_reads_rRNA', 'rRNA_percentage']].to_csv(output.tsv, sep='\t')
        y = {
            'id': 'rrna_percentages_table',
            'section_name': 'rRNA content',
            'description': 'Amount of reads mapping to rRNA sequence',
            'plot_type': 'table',
            'pconfig': {
                'id': 'rrna_percentages_table_table',
                'title': 'rRNA content table',
                'min': 0
            },
            'data': yaml.load(df.transpose().to_json(), Loader=yaml.FullLoader),
        }
        with open(output.json, 'w') as fout:
            yaml.dump(y, fout, default_flow_style=False)


rule libsizes_table:
    """
    Aggregate fastq and bam counts in to a single table
    """
    input:
        utils.flatten(c.targets['libsizes'])
    output:
        json=c.patterns['libsizes_yaml'],
        tsv=c.patterns['libsizes_table']
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 2
    run:
        def sample(f):
            return os.path.basename(os.path.dirname(f))

        def million(f):
            return float(open(f).read()) / 1e6

        def stage(f):
            return os.path.basename(f).split('.', 1)[1].replace('.gz', '').replace('.count', '')

        df = pd.DataFrame(dict(filename=list(map(str, input))))
        df['sample'] = df.filename.apply(sample)
        df['million'] = df.filename.apply(million)
        df['stage'] = df.filename.apply(stage)
        df = df.set_index('filename')
        df = df.pivot('sample', columns='stage', values='million')

        # make nicer column names
        convert = {
            'fastq.libsize': 'stage1_raw',
            'cutadapt.fastq.libsize' : 'stage2_trimmed',
            'cutadapt.bam.libsize': 'stage3_aligned',
        }

        df.columns = [convert[i] for i in df.columns]

        df.to_csv(output.tsv, sep='\t')
        y = {
            'id': 'libsizes_table',
            'section_name': 'Library sizes',
            'description': 'Library sizes at various stages of the pipeline',
            'plot_type': 'table',
            'pconfig': {
                'id': 'libsizes_table_table',
                'title': 'Library size table',
                'min': 0
            },
            'data': yaml.load(df.transpose().to_json(), Loader=yaml.FullLoader),
        }
        with open(output.json, 'w') as fout:
            yaml.dump(y, fout, default_flow_style=False)


rule multiqc:
    """
    Aggregate various QC stats and logs into a single HTML report with MultiQC
    """
    # NOTE: if you add more rules and want MultiQC to pick up the output, then
    # add outputs from those rules to the inputs here.
    input:
        files=(
            utils.flatten(c.targets['fastqc']) +
            utils.flatten(c.targets['libsizes_yaml']) +
            utils.flatten(c.targets['rrna_percentages_yaml']) +
            utils.flatten(c.targets['cutadapt']) +
            utils.flatten(c.targets['featurecounts']) +
            utils.flatten(c.targets['bam']) +
            utils.flatten(c.targets['markduplicates']) +
            utils.flatten(c.targets['salmon']) +
            utils.flatten(c.targets['rseqc']) +
            utils.flatten(c.targets['fastq_screen']) +
            utils.flatten(c.targets['dupradar']) +
            utils.flatten(c.targets['preseq']) +
            utils.flatten(c.targets['collectrnaseqmetrics'])
        ),
        config='config/multiqc_config.yaml'
    output: c.targets['multiqc']
    log: c.targets['multiqc'][0] + '.log'
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 2
    run:
        analysis_directory = set([os.path.dirname(i) for i in input])
        outdir = os.path.dirname(c.targets['multiqc'][0])
        basename = os.path.basename(c.targets['multiqc'][0])
        shell(
            'LC_ALL=en_US.UTF.8 LC_LANG=en_US.UTF-8 '
            'multiqc '
            '--quiet '
            '--outdir {outdir} '
            '--force '
            '--filename {basename} '
            '--config {input.config} '
            '{analysis_directory} '
            '&> {log} '
        )

rule markduplicates:
    """
    Mark or remove PCR duplicates with Picard MarkDuplicates
    """
    input:
        bam=c.patterns['bam']
    output:
        bam=c.patterns['markduplicates']['bam'],
        metrics=c.patterns['markduplicates']['metrics']
    log:
        c.patterns['markduplicates']['bam'] + '.log'
    params:
        # NOTE: Be careful with the memory here; make sure you have enough
        # and/or it matches the resources you're requesting in the cluster
        # config.
        java_args='-Xmx20g'
        # java_args='-Xmx2g'  # [TEST SETTINGS -1]
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 20,
        time_hr=lambda wildcards, attempt: attempt * 2
    shell:
        'picard '
        '{params.java_args} '
        'MarkDuplicates '
        'INPUT={input.bam} '
        'OUTPUT={output.bam} '
        'METRICS_FILE={output.metrics} '
        '&> {log}'


rule collectrnaseqmetrics:
    """
    Calculate various RNA-seq QC metrics with Picarc CollectRnaSeqMetrics
    """
    input:
        bam=c.patterns['bam'],
        refflat=c.refdict[c.organism][config['gtf']['tag']]['refflat']
    output:
        metrics=c.patterns['collectrnaseqmetrics']['metrics'],
        pdf=c.patterns['collectrnaseqmetrics']['pdf']
    params:
        # NOTE: Be careful with the memory here; make sure you have enough
        # and/or it matches the resources you're requesting in the cluster
        # config.
        java_args='-Xmx20g'
        # java_args='-Xmx2g'  # [TEST SETTINGS -1]
    log:
        c.patterns['collectrnaseqmetrics']['metrics'] + '.log'
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 20,
        time_hr=lambda wildcards, attempt: attempt * 2
    shell:
        'picard '
        '{params.java_args} '
        'CollectRnaSeqMetrics '

        # NOTE: Adjust strandedness appropriately. From the Picard docs:
        #
        #     STRAND=StrandSpecificity For strand-specific library prep. For
        #     unpaired reads, use FIRST_READ_TRANSCRIPTION_STRAND if the reads
        #     are expected to be on the transcription strand.  Required.
        #     Possible values: {NONE, FIRST_READ_TRANSCRIPTION_STRAND,
        #     SECOND_READ_TRANSCRIPTION_STRAND}
        #
        'STRAND=SECOND_READ_TRANSCRIPTION_STRAND '

        'CHART_OUTPUT={output.pdf} '
        'REF_FLAT={input.refflat} '
        'INPUT={input.bam} '
        'OUTPUT={output.metrics} '
        '&> {log}'


rule preseq:
    """
    Compute a library complexity curve with preseq
    """
    input:
        bam=c.patterns['bam']
    output:
        c.patterns['preseq']
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 8,
        time_hr=lambda wildcards, attempt: attempt * 2
    shell:
        'preseq '
        'c_curve '
        '-B {input} '
        '-o {output} '


rule dupRadar:
    """
    Assess the library complexity with dupRadar
    """
    input:
        bam=rules.markduplicates.output.bam,
        annotation=c.refdict[c.organism][config['gtf']['tag']]['gtf'],
    output:
        density_scatter=c.patterns['dupradar']['density_scatter'],
        expression_histogram=c.patterns['dupradar']['expression_histogram'],
        expression_boxplot=c.patterns['dupradar']['expression_boxplot'],
        expression_barplot=c.patterns['dupradar']['expression_barplot'],
        multimapping_histogram=c.patterns['dupradar']['multimapping_histogram'],
        dataframe=c.patterns['dupradar']['dataframe'],
        model=c.patterns['dupradar']['model'],
        curve=c.patterns['dupradar']['curve'],
    log: c.patterns['dupradar']['dataframe'] + '.log'
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 8,
        time_hr=lambda wildcards, attempt: attempt * 2
    script:
        wrapper_for('dupradar/wrapper.py')


rule salmon:
    """
    Quantify reads coming from transcripts with Salmon
    """
    input:
        fastq=common.fill_r1_r2(c.sampletable, c.patterns['cutadapt']),
        index=c.refdict[c.organism][config['salmon']['tag']]['salmon'],
    output:
        c.patterns['salmon']
    params:
        index_dir=os.path.dirname(c.refdict[c.organism][config['salmon']['tag']]['salmon']),
        outdir=os.path.dirname(c.patterns['salmon'])
    log:
        c.patterns['salmon'] + '.log'
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 2,
        time_hr=lambda wildcards, attempt: attempt * 4
    run:
        paired = len(input.fastq) == 2
        if paired:
            shell(
                # NOTE: adjust Salmon params as needed
                'salmon quant '
                '--index {params.index_dir} '
                '--output {params.outdir} '
                '--threads {threads} '

                # NOTE: --libType=A auto-detects library type. Change if needed.
                '--libType=A '

                # NOTE: Docs suggest using --gcBias and --seqBias is a good idea
                '--gcBias '
                '--seqBias '
                '-1 {input.fastq[0]} '
                '-2 {input.fastq[1]} '
                '&> {log}'
            )
        else:
            shell(
                # NOTE: adjust Salmon params as needed
                'salmon quant '
                '--index {params.index_dir} '
                '--output {params.outdir} '
                '--threads {threads} '

                # NOTE: --libType=A auto-detects library type. Change if needed.
                '--libType=A '

                # NOTE: Docs suggest using --gcBias and --seqBias is a good idea
                '--gcBias '
                '--seqBias '
                '-r {input.fastq} '
                '&> {log}'
            )


rule rseqc_bam_stat:
    """
    Calculate various BAM stats with RSeQC
    """
    input:
        bam=c.patterns['bam']
    output:
        txt=c.patterns['rseqc']['bam_stat']
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 2
    shell:
        'bam_stat.py -i {input.bam} > {output.txt}'


rule bigwig_neg:
    """
    Create a bigwig for negative-strand reads
    """
    input:
        bam=c.patterns['bam'],
        bai=c.patterns['bam'] + '.bai',
    output: c.patterns['bigwig']['neg']
    threads: 8
    log:
        c.patterns['bigwig']['neg'] + '.log'
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 8,
        time_hr=lambda wildcards, attempt: attempt * 2
    shell:
        # NOTE: adjust bamCoverage params as needed
        # Make sure the bigwig_pos rule below reflects the same changes.
        'bamCoverage '
        '--bam {input.bam} '
        '-o {output} '
        '-p {threads} '
        '--minMappingQuality 20 '
        '--ignoreDuplicates '
        '--smoothLength 10 '
        '--filterRNAstrand forward '
        '--normalizeUsing BPM '
        '&> {log}'


rule bigwig_pos:
    """
    Create a bigwig for postive-strand reads.
    """
    input:
        bam=c.patterns['bam'],
        bai=c.patterns['bam'] + '.bai',
    output: c.patterns['bigwig']['pos']
    threads: 8
    log:
        c.patterns['bigwig']['pos'] + '.log'
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 8,
        time_hr=lambda wildcards, attempt: attempt * 2
    shell:
        # NOTE: adjust bamCoverage params as needed
        # Make sure the bigwig_neg rule above reflects the same changes.
        'bamCoverage '
        '--bam {input.bam} '
        '-o {output} '
        '-p {threads} '
        '--minMappingQuality 20 '
        '--ignoreDuplicates '
        '--smoothLength 10 '
        '--filterRNAstrand reverse '
        '--normalizeUsing BPM '
        '&> {log}'


rule symlink_bigwigs:
    input:
        pos=c.patterns['bigwig']['pos'],
        neg=c.patterns['bigwig']['neg'],
    output:
        sense=c.patterns['bigwig']['sense'],
        antisense=c.patterns['bigwig']['antisense'],
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 1
    run:
        # NOTE:
        #    In our test data, reads mapping to the positive strand correspond
        #    to sense-strand transcripts. If your protocol is reversed (e.g.,
        #    TruSeq kits), you can map negative-strand reads to "sense". The
        #    track hub creation in rnaseq_trackhub.py only cares about the
        #    `sense` and `antisense` versions, so it's cheap to mess around
        #    with the symlinking here.
        #
        # E.g., use this for TruSeq:
        # utils.make_relative_symlink(input.pos, output.antisense)
        # utils.make_relative_symlink(input.neg, output.sense)

        utils.make_relative_symlink(input.pos, output.sense)
        utils.make_relative_symlink(input.neg, output.antisense)

################################################################################
# MY CUSTOM RULES
################################################################################

rule intergenic:
    input:
         db=c.refdict[c.organism][config['db']['tag']]['gtf'] + '.db'
    output:
          bed=c.patterns['intergenic']['bed'],
          gtf=c.patterns['intergenic']['gtf']
    resources:
             mem_gb=lambda wildcards, attempt: attempt * 1,
             time_hr=lambda wildcards, attempt: attempt * 1
    run:
        import gffutils
        from gffutils.pybedtools_integration import to_bedtool, featurefuncs

        db = gffutils.FeatureDB(input.db)
        gene = to_bedtool(db.features_of_type('gene')).saveas()
        slopped = gene.slop(b=100, genome='dm6')
        merged = slopped.sort().merge()
        complement = merged.complement(genome='dm6').saveas()

        global cnt
        cnt = 1


        def interName(feature):
            global cnt
            feature = featurefuncs.extend_fields(feature, 4)
            feature.name = 'intergenic{}'.format(cnt)
            cnt += 1
            return feature


        def interGFF(feature):
            gff = featurefuncs.bed2gff(feature)
            gff[1] = 'bedtools'
            gff[2] = 'exon'
            gff.attrs['gene_id'] = gff.name
            return gff


        bed = complement.each(interName).saveas(output.bed)
        bed.each(interGFF).saveas(output.gtf)

rule featurecounts_intergenic:
    input:
        annotation = c.patterns['intergenic']['gtf'],
        bam = rules.hisat2.output
    output: counts = c.patterns['featurecounts_intergenic']
    log: c.patterns['featurecounts_intergenic'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell:
        'featureCounts '
        '-T {threads} '
        '-a {input.annotation} '
        '-o {output.counts} '
        '{input.bam} '
        '&> {log}'

rule gene_counts_agg:
    input: c.targets['featurecounts']['s2']
    output: c.targets['agg_featurecounts']
    resources:
             mem_gb = lambda wildcards, attempt: attempt * 1,
             time_hr = lambda wildcards, attempt: attempt * 2
    run:
        dfs = []
        for f in input:
            fname = Path(f)
            sname = fname.name.split('.')[0]
            df = pd.read_csv(fname, sep='\t', comment='#', index_col=0)
            df = df.iloc[:, -1]
            df.name = sname
            dfs.append(df)

        data = pd.concat(dfs, axis=1, sort=True)
        data.to_csv(output[0], sep="\t")

rule intergenic_counts_agg:
    input: c.targets['featurecounts_intergenic']
    output: c.targets['agg_intergenic_featurecounts']
    resources:
             mem_gb = lambda wildcards, attempt: attempt * 1,
             time_hr = lambda wildcards, attempt: attempt * 2
    run:
        dfs = []
        for f in input:
            fname = Path(f)
            sname = fname.name.split('.')[0]
            df = pd.read_csv(fname, sep='\t', comment='#', index_col=0)
            df = df.iloc[:, -1]
            df.name = sname
            dfs.append(df)

        data = pd.concat(dfs, axis=1, sort=True)
        data.to_csv(output[0], sep="\t")

rule gene_lengths:
    input: c.refdict[c.organism][config['gtf']['tag']]['gtf']
    output: c.targets['gene_lengths']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    run:
        from larval_gonad.io import GffRow

        res = ["Geneid\tlength"]
        with open(input[0]) as fh:
            for row in fh.read().strip('\n').split('\n'):
                try:
                    gffrow = GffRow(row)
                except ValueError as e:
                    pass

                gene_id = gffrow.parsed_attributes['gene_id']
                length = int(gffrow.end) - int(gffrow.start)

                if gffrow.is_gene | gene_id.startswith("ERCC"):
                    res.append(f"{gene_id}\t{length}")

        with open(output[0], 'w') as fh:
            fh.write('\n'.join(res) + "\n")

rule ercc_annotation:
    output: c.targets['ercc_annotation']
    resources:
             mem_gb = lambda wildcards, attempt: attempt * 1,
             time_hr = lambda wildcards, attempt: attempt * 2
    run:
        cols = ['ERCC_Control', 'Subpool_in_pool_78', '78A_nmol_per_ul', '78B_nmol_per_ul']
        (
            pd.read_csv("http://www.jgenomics.com/v04/p0019/jgenv04p0019s1.csv", index_col=0, usecols=cols)
            .rename_axis('ercc_id')
            .to_csv(output[0], sep='\t')
        )

rule tpm:
    input:
         cnts=rules.gene_counts_agg.output[0],
         lens=c.targets['gene_lengths'][0].format(organism=config['organism'], tag=config['gtf']['tag'])
    output: c.targets['tpm_featurecounts']
    resources:
             mem_gb = lambda wildcards, attempt: attempt * 1,
             time_hr = lambda wildcards, attempt: attempt * 2
    run:
        from larval_gonad.normalization import tpm

        raw = pd.read_csv(input.cnts, sep='\t').set_index('Geneid')
        lens = pd.read_csv(input.lens, sep='\t').set_index('Geneid')['length']
        lens = lens.reindex(raw.index)

        res = tpm(raw, lens).dropna()
        res.to_csv(output[0], sep='\t')

rule differential_expression:
    input:
        sample_table = 'config/sampletable.tsv',
        counts_table = rules.gene_counts_agg.output[0],
        intergenic_counts_table = rules.intergenic_counts_agg.output[0],
        gene_annot = '../references/gene_annotation_dmel_r6-24.feather',
        ercc_annot = '../references/ercc_annotation.tsv'
    output: '../output/bulk2-rnaseq-wf/deg/bulk_testis_vs_ovary.tsv'
    log: '../output/bulk2-rnaseq-wf/deg/bulk_testis_vs_ovary.log'
    conda: "../envs/deseq2.yaml"
    script: "scripts/testis_biased_expression.R"

rule gonad_biased_fbgns:
    input: rules.differential_expression.output[0]
    output: 
        male='../output/bulk2-rnaseq-wf/deg/larval_testis_biased_fbgns.pkl',
        female='../output/bulk2-rnaseq-wf/deg/larva_ovary_biased_fbgns.pkl',
        ns='../output/bulk2-rnaseq-wf/deg/larval_ns_biased_fbgns.pkl',
    run:
        from larval_gonad.io import pickle_dump
        df = pd.read_csv(input[0], sep='\t').set_index("FBgn")
        pickle_dump(df.query(f"padj <= 0.01 & log2FoldChange > 0").index.tolist(), output['male'])
        pickle_dump(df.query(f"padj <= 0.01 & log2FoldChange < 0").index.tolist(), output['female'])
        pickle_dump(df.query(f"padj > 0.01").index.tolist(), output['ns'])

import sys
import os
import yaml
from pathlib import Path

import pandas as pd

from lcdblib.snakemake import helpers, aligners
from lcdblib.utils import utils

sys.path.insert(0, srcdir('../lcdb-wf'))
from lib import common
from lib.patterns_targets import RNASeqConfig

# ----------------------------------------------------------------------------
# Note:
#
#  Search this file for the string "# TEST SETTINGS" and make necessary edits
#  before running this on real data.
# ----------------------------------------------------------------------------
configfile: 'config/config.yaml'

shell.prefix('set -euo pipefail; export TMPDIR={};'.format(common.tempdir_for_biowulf()))
shell.executable('/bin/bash')
refdict, conversion_kwargs = common.references_dict(config)

c = RNASeqConfig(config, 'config/rnaseq_patterns.yaml')


def wrapper_for(path):
    return 'file:' + os.path.join('../lcdb-wf', 'wrappers', 'wrappers', path)


# ----------------------------------------------------------------------------
# RULES
# ----------------------------------------------------------------------------

rule targets:
    input:
        c.targets['bam'] +
        c.targets['agg_featurecounts'] +
        c.targets['tpm_featurecounts'] +
        utils.flatten(c.targets['fastqc']) +
        utils.flatten(c.targets['libsizes']) +
        [c.targets['fastq_screen']] +
        [c.targets['libsizes_table']] +
        [c.targets['rrna_percentages_table']] +
        [c.targets['multiqc']] +
        utils.flatten(c.targets['featurecounts']) +
        utils.flatten(c.targets['featurecounts_intergenic']) +
        utils.flatten(c.targets['rrna']) +
        utils.flatten(c.targets['markduplicates']) +
        utils.flatten(c.targets['salmon']) +
        utils.flatten(c.targets['collectrnaseqmetrics']) +
        utils.flatten(c.targets['bigwig']) +
        utils.flatten(c.targets['downstream']) +
        utils.flatten(c.targets['male-bias'])


if 'orig_filename' in c.sampletable.columns:
    """Symlinks files over from original filename"""
    rule symlinks:
        input: lambda wc: c.sampletable.set_index(c.sampletable.columns[0])['orig_filename'].to_dict()[wc.sample]
        output: c.patterns['fastq']
        resources:
             mem_gb=lambda wildcards, attempt: attempt * 1,
             time_hr=lambda wildcards, attempt: attempt * 1
        run:
            utils.make_relative_symlink(input[0], output[0])

    rule symlink_targets:
        input: c.targets['fastq']
        resources:
                 mem_gb=lambda wildcards, attempt: attempt * 1,
                 time_hr=lambda wildcards, attempt: attempt * 1

rule cutadapt:
    input: fastq=c.patterns['fastq']
    output: fastq=c.patterns['cutadapt']
    log: c.patterns['cutadapt'] + '.log'
    params: extra='-a file:../lcdb-wf/include/adapters.fa -q 20 --minimum-length=25'
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 2
    script: '../lcdb-wf/wrappers/wrappers/cutadapt/wrapper.py'

rule fastqc:
    input: '{sample_dir}/{sample}/{sample}{suffix}'
    output:
        html = '{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.html',
        zip = '{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.zip'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    script: '../lcdb-wf/wrappers/wrappers/fastqc/wrapper.py'

rule hisat2_splice_site:
    input: gtf = c.refdict[c.assembly][config['gtf']['tag']]['gtf']
    output: c.patterns['splice_sites']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"

rule hisat2:
    input:
        fastq = rules.cutadapt.output.fastq,
        index = [c.refdict[c.assembly][config['aligner']['tag']]['hisat2']]
    output: bam = c.patterns['bam']
    log: c.patterns['bam'] + '.log'
    params:
        hisat2_extra = '--max-intronlen 300000 --known-splicesite-infile {splice} --rna-strandness R'.format(splice=c.patterns['splice_sites']),
        samtools_view_extra = "--threads 4 -q 20",
        samtools_sort_extra = '--threads 4 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 8,
        time_hr = lambda wildcards, attempt: attempt * 4
    threads: 6
    script: '../lcdb-wf/wrappers/wrappers/hisat2/align/wrapper.py'

rule rRNA:
    input:
        fastq = rules.cutadapt.output.fastq,
        index = [c.refdict[c.assembly][config['rrna']['tag']]['bowtie2']]
    output: bam = c.patterns['rrna']['bam']
    log: c.patterns['rrna']['bam'] + '.log'
    params: samtools_view_extra = "--threads 4 -q 20"
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    threads: 6
    script: '../lcdb-wf/wrappers/wrappers/bowtie2/align/wrapper.py'

rule fastq_count:
    input: fastq = '{sample_dir}/{sample}/{sample}{suffix}.fastq.gz'
    output: count = '{sample_dir}/{sample}/{sample}{suffix}.fastq.gz.libsize'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell: 'zcat {input} | echo $((`wc -l`/4)) > {output}'

rule bam_count:
    input: bam = '{sample_dir}/{sample}/{suffix}.bam'
    output: count = '{sample_dir}/{sample}/{suffix}.bam.libsize'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell: 'samtools view -c {input} > {output}'

rule bam_index:
    input: bam = '{prefix}.bam'
    output: bai = '{prefix}.bam.bai'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell: 'samtools index {input} {output}'

rule fastq_screen:
    input:
        fastq = rules.cutadapt.output.fastq,
        dm6 = c.refdict['dmel'][config['aligner']['tag']]['bowtie2'],
        rRNA = c.refdict[c.assembly][config['rrna']['tag']]['bowtie2'],
        phix = c.refdict['phix']['default']['bowtie2'],
        ercc = refdict['ercc']['srm2374']['bowtie2'],
        hg19 = refdict['human']['gencode-v19']['bowtie2'],
        wolbachia = refdict['wolbachia']['default']['bowtie2'],
        ecoli = refdict['ecoli']['default']['bowtie2'],
        yeast = refdict['sacCer3']['default']['bowtie2']
    output: txt = c.patterns['fastq_screen']
    log: c.patterns['fastq_screen'] + '.log'
    params: subset = 100000
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 8,
        time_hr = lambda wildcards, attempt: attempt * 2
    script: '../lcdb-wf/wrappers/wrappers/fastq_screen/wrapper.py'

rule featurecounts:
    input:
        annotation = c.refdict[c.assembly][config['gtf']['tag']]['gtf'],
        bam = rules.hisat2.output
    output: counts = c.patterns['featurecounts']
    log: c.patterns['featurecounts'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell:
        'featureCounts '
        '-T {threads} '
        '-a {input.annotation} '
        '-s 2 '
        '-o {output.counts} '
        '{input.bam} '
        '&> {log}'

rule rrna_libsizes_table:
    input:
        rrna = c.targets['rrna']['libsize'],
        fastq = c.targets['libsizes']['cutadapt']
    output:
        json = c.patterns['rrna_percentages_yaml'],
        tsv = c.patterns['rrna_percentages_table']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    run:
        def rrna_sample(f):
            return helpers.extract_wildcards(c.patterns['rrna']['libsize'], f)['sample']

        def sample(f):
            return helpers.extract_wildcards(c.patterns['libsizes']['cutadapt'], f)['sample']

        def million(f):
            return float(open(f).read()) / 1e6


        rrna = sorted(input.rrna, key=rrna_sample)
        fastq = sorted(input.fastq, key=sample)
        samples = list(map(rrna_sample, rrna))
        rrna_m = list(map(million, rrna))
        fastq_m = list(map(million, fastq))

        df = pd.DataFrame(dict(
            sample=samples,
            million_reads_rRNA=rrna_m,
            million_reads_fastq=fastq_m,
        ))
        df = df.set_index('sample')
        df['rRNA_percentage'] = df.million_reads_rRNA / df.million_reads_fastq * 100

        df[['million_reads_fastq', 'million_reads_rRNA', 'rRNA_percentage']].to_csv(output.tsv, sep='\t')
        y = {
            'id': 'rrna_percentages_table',
            'section_name': 'rRNA content',
            'description': 'Amount of reads mapping to rRNA sequence',
            'plot_type': 'table',
            'pconfig': {
                'id': 'rrna_percentages_table_table',
                'title': 'rRNA content table',
                'min': 0
            },
            'data': yaml.load(df.transpose().to_json()),
        }
        with open(output.json, 'w') as fout:
            yaml.dump(y, fout, default_flow_style=False)

rule libsizes_table:
    input: utils.flatten(c.targets['libsizes'])
    output:
        json = c.patterns['libsizes_yaml'],
        tsv = c.patterns['libsizes_table']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    run:
        def sample(f):
            return os.path.basename(os.path.dirname(f))


        def million(f):
            return float(open(f).read()) / 1e6


        def stage(f):
            return os.path.basename(f).split('.', 1)[1].replace('.gz', '').replace('.count', '')


        df = pd.DataFrame(dict(filename=list(map(str, input))))
        df['sample'] = df.filename.apply(sample)
        df['million'] = df.filename.apply(million)
        df['stage'] = df.filename.apply(stage)
        df = df.set_index('filename')
        df = df.pivot('sample', columns='stage', values='million')

        # make nicer column names
        convert = {
            'fastq.libsize': 'stage1_raw',
            'cutadapt.fastq.libsize': 'stage2_trimmed',
            'cutadapt.bam.libsize': 'stage3_aligned',
        }

        df.columns = [convert[i] for i in df.columns]

        df.to_csv(output.tsv, sep='\t')
        y = {
            'id': 'libsizes_table',
            'section_name': 'Library sizes',
            'description': 'Library sizes at various stages of the pipeline',
            'plot_type': 'table',
            'pconfig': {
                'id': 'libsizes_table_table',
                'title': 'Library size table',
                'min': 0
            },
            'data': yaml.load(df.transpose().to_json()),
        }
        with open(output.json, 'w') as fout:
            yaml.dump(y, fout, default_flow_style=False)

rule multiqc:
    input:
        files = (
                        utils.flatten(c.targets['fastqc']) +
                        utils.flatten(c.targets['libsizes_yaml']) +
                        utils.flatten(c.targets['rrna_percentages_yaml']) +
                        utils.flatten(c.targets['cutadapt']) +
                        utils.flatten(c.targets['featurecounts']) +
                        utils.flatten(c.targets['bam']) +
                        utils.flatten(c.targets['markduplicates']) +
                        utils.flatten(c.targets['salmon']) +
                        utils.flatten(c.targets['fastq_screen']) +
                        utils.flatten(c.targets['collectrnaseqmetrics'])
                ),
        config = 'config/multiqc_config.yaml'
    output: c.targets['multiqc']
    params:
        analysis_directory = " ".join([c.sample_dir, c.agg_dir]),
        extra = '--config config/multiqc_config.yaml',
        outdir = os.path.dirname(c.targets['multiqc'][0]),
        basename = os.path.basename(c.targets['multiqc'][0])
    log: c.targets['multiqc'][0] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell:
        'LC_ALL=en_US.UTF.8 LC_LANG=en_US.UTF-8 '
        'multiqc '
        '--quiet '
        '--outdir {params.outdir} '
        '--force '
        '--filename {params.basename} '
        '--config config/multiqc_config.yaml '
        '{params.analysis_directory} '
        '&> {log} '

rule markduplicates:
    input: bam = rules.hisat2.output
    output:
        bam = c.patterns['markduplicates']['bam'],
        metrics = c.patterns['markduplicates']['metrics']
    log: c.patterns['markduplicates']['bam'] + '.log'
    params: java_args = '-Xmx12g'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 12,
        time_hr = lambda wildcards, attempt: attempt * 4
    shell:
        'picard '
        '{params.java_args} '
        'MarkDuplicates '
        'INPUT={input.bam} '
        'OUTPUT={output.bam} '
        'METRICS_FILE={output.metrics} '
        '&> {log}'

rule collectrnaseqmetrics:
    input:
        bam = c.patterns['bam'],
        refflat = c.refdict[c.assembly][config['gtf']['tag']]['refflat']
    output:
        metrics = c.patterns['collectrnaseqmetrics']['metrics'],
        pdf = c.patterns['collectrnaseqmetrics']['pdf']
    params: java_args = '-Xmx12g'
    log: c.patterns['collectrnaseqmetrics']['metrics'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 12,
        time_hr = lambda wildcards, attempt: attempt * 4
    shell:
        'picard '
        '{params.java_args} '
        'CollectRnaSeqMetrics '
        'STRAND=SECOND_READ_TRANSCRIPTION_STRAND CHART_OUTPUT={output.pdf} '
        'REF_FLAT={input.refflat} '
        'INPUT={input.bam} '
        'OUTPUT={output.metrics} '
        '&> {log}'

rule salmon:
    input:
        fastq = c.patterns['cutadapt'],
        index = c.refdict[c.assembly][config['salmon']['tag']]['salmon'],
    output: c.patterns['salmon']
    params:
        index_dir = os.path.dirname(c.refdict[c.assembly][config['salmon']['tag']]['salmon']),
        outdir = os.path.dirname(c.patterns['salmon'])
    log: c.patterns['salmon'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 4
    shell:
        'salmon quant '
        '--index {params.index_dir} '
        '--output {params.outdir} '
        '--threads {threads} '
        '--libType=A '
        '-r {input.fastq} '
        '&> {log}'

rule bigwig_neg:
    input:
        bam = c.patterns['bam'],
        bai = c.patterns['bam'] + '.bai'
    output: c.patterns['bigwig']['neg']
    threads: 8
    log: c.patterns['bigwig']['neg'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 8,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell:
        'bamCoverage '
        '--bam {input.bam} '
        '-o {output} '
        '-p {threads} '
        '--minMappingQuality 20 '
        '--ignoreDuplicates '
        '--smoothLength 10 '
        '--filterRNAstrand forward '
        '--normalizeUsing RPKM '
        '&> {log}'

rule bigwig_pos:
    input:
        bam = c.patterns['bam'],
        bai = c.patterns['bam'] + '.bai',
    output: c.patterns['bigwig']['pos']
    threads: 8
    log: c.patterns['bigwig']['pos'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell:
        'bamCoverage '
        '--bam {input.bam} '
        '-o {output} '
        '-p {threads} '
        '--minMappingQuality 20 '
        '--ignoreDuplicates '
        '--smoothLength 10 '
        '--filterRNAstrand reverse '
        '--normalizeUsing RPKM '
        '&> {log}'

rule rnaseq_rmarkdown:
    input:
        featurecounts = c.targets['featurecounts'],
        rmd = 'downstream/rnaseq.Rmd',
        sampletable = config['sampletable']
    output: c.targets['downstream']['rnaseq']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell: """
            Rscript -e "rmarkdown::render('{input.rmd}', 'knitrBootstrap::bootstrap_document')" && \
            mv downstream/rnaseq.html {output[0]}
        """

rule intergenic:
    input:
         db=c.refdict[c.assembly][config['db']['tag']]['gtf'] + '.db'
    output:
          bed=c.patterns['intergenic']['bed'],
          gtf=c.patterns['intergenic']['gtf']
    resources:
             mem_gb=lambda wildcards, attempt: attempt * 1,
             time_hr=lambda wildcards, attempt: attempt * 1
    run:
        import gffutils
        from gffutils.pybedtools_integration import to_bedtool, featurefuncs

        db = gffutils.FeatureDB(input.db)
        gene = to_bedtool(db.features_of_type('gene')).saveas()
        slopped = gene.slop(b=100, genome='dm6')
        merged = slopped.sort().merge()
        complement = merged.complement(genome='dm6').saveas()

        global cnt
        cnt = 1


        def interName(feature):
            global cnt
            feature = featurefuncs.extend_fields(feature, 4)
            feature.name = 'intergenic{}'.format(cnt)
            cnt += 1
            return feature


        def interGFF(feature):
            gff = featurefuncs.bed2gff(feature)
            gff[1] = 'bedtools'
            gff[2] = 'exon'
            gff.attrs['gene_id'] = gff.name
            return gff


        bed = complement.each(interName).saveas(output.bed)
        bed.each(interGFF).saveas(output.gtf)

rule featurecounts_intergenic:
    input:
        annotation = c.patterns['intergenic']['gtf'],
        bam = rules.hisat2.output
    output: counts = c.patterns['featurecounts_intergenic']
    log: c.patterns['featurecounts_intergenic'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 2
    shell:
        'featureCounts '
        '-T {threads} '
        '-a {input.annotation} '
        '-o {output.counts} '
        '{input.bam} '
        '&> {log}'

rule gene_counts_agg:
    input: c.targets['featurecounts']
    output: c.targets['agg_featurecounts']
    resources:
             mem_gb = lambda wildcards, attempt: attempt * 1,
             time_hr = lambda wildcards, attempt: attempt * 2
    run:
        dfs = []
        for f in input:
            fname = Path(f)
            sname = fname.name.split('.')[0]
            df = pd.read_csv(fname, sep='\t', comment='#', index_col=0)
            df = df.iloc[:, -1]
            df.name = sname
            dfs.append(df)

        data = pd.concat(dfs, axis=1, sort=True)
        data.to_parquet(output[0])

rule tpm:
    input:
         cnts=rules.gene_counts_agg.output[0],
         lens='../output/gene_ts_lengths.tsv'
    output: c.targets['tpm_featurecounts']
    resources:
             mem_gb = lambda wildcards, attempt: attempt * 1,
             time_hr = lambda wildcards, attempt: attempt * 2
    run:
        from larval_gonad.normalization import tpm

        raw = pd.read_parquet(input.cnts)
        lens = pd.read_csv(input.lens, sep='\t').set_index('FBgn').gene_ts_length
        lens = lens.reindex(raw.index)

        res = tpm(raw, lens).dropna()
        res.to_parquet(output[0])

rule male_biased_expression:
    input: c.targets['downstream']['tcp_table']
    output: c.targets['male-bias']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    run:
        male_bias = (
            pd.read_csv(input[0], sep='\t', index_col=0)
            .rename_axis('FBgn')
            .query('padj <= 0.01 & log2FoldChange >= 1')
            .index.tolist()
        )
        with open(output[0], 'w') as fh:
            fh.write('\n'.join(male_bias))

# vim: ft=snakemake.python
# vim: foldmethod=indent
